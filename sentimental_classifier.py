# -*- coding: utf-8 -*-
"""sentimental classifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18VmTv_npiFwyZkQZ9fU70F83esdq1AAd
"""

!tar -xzvf review_polarity.tar.gz

# @title Importing all required packages
import os
import nltk
import numpy as np
import pandas as pd
from sklearn.svm import SVC
from collections import Counter
from nltk.corpus import opinion_lexicon
from nltk.tokenize import word_tokenize
nltk.download('punkt')
nltk.download('opinion_lexicon')

# @title making data in a table of frequency of postive and negative unigrams
def calculate_freq(text, positive_criteria, negative_criteria):
    tokens = word_tokenize(text)
    pos_freq = sum(1 for token in tokens if token.lower() in positive_criteria)
    neg_freq = sum(1 for token in tokens if token.lower() in negative_criteria)
    return pos_freq, neg_freq

# Paths to your positive and negative folders
positive_folder = '/content/txt_sentoken/pos'
negative_folder = '/content/txt_sentoken/neg'

# Use NLTK's opinion_lexicon positive and negative words as criteria
positive_criteria = set(opinion_lexicon.positive())
negative_criteria = set(opinion_lexicon.negative())

# Create lists to store positive and negative frequencies for each document
positive_freq_per_doc = []
negative_freq_per_doc = []
class_labels = []

# Calculate positive and negative frequencies for each document in the positive folder
for file_name in os.listdir(positive_folder):
    with open(os.path.join(positive_folder, file_name), 'r', encoding='utf-8') as file:
        content = file.read()
        pos_freq, neg_freq = calculate_freq(content, positive_criteria, negative_criteria)
        positive_freq_per_doc.append(pos_freq)
        negative_freq_per_doc.append(neg_freq)
        class_labels.append(1)  # Class label 1 for positive documents

# Calculate positive and negative frequencies for each document in the negative folder
for file_name in os.listdir(negative_folder):
    with open(os.path.join(negative_folder, file_name), 'r', encoding='utf-8') as file:
        content = file.read()
        pos_freq, neg_freq = calculate_freq(content, positive_criteria, negative_criteria)
        positive_freq_per_doc.append(pos_freq)
        negative_freq_per_doc.append(neg_freq)
        class_labels.append(0)  # Class label 0 for negative documents

#normalizing them
positive_freq_per_doc_np = np.array(positive_freq_per_doc)
negative_freq_per_doc_np = np.array(negative_freq_per_doc)
positive_freq_per_doc_nor = positive_freq_per_doc_np / (positive_freq_per_doc_np + negative_freq_per_doc_np)
negative_freq_per_doc_nor = negative_freq_per_doc_np / (positive_freq_per_doc_np + negative_freq_per_doc_np)

# Combine positive and negative frequencies and class labels into a DataFrame
data = {
    'Positive_Freq': positive_freq_per_doc_nor,
    'Negative_Freq': negative_freq_per_doc_nor,
    'Class': class_labels
}
df = pd.DataFrame(data)

# Display the DataFrame
print(df)

# @title naive bayes with freq
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

X = df[['Positive_Freq', 'Negative_Freq']]
y = df['Class']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify = y)

# Creating a Multinomial Naive Bayes classifier
nb = MultinomialNB()

# Training the Naive Bayes model
nb.fit(X_train, y_train)

# Predicting on the test set
y_pred = nb.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Classification report
print(classification_report(y_test, y_pred))

# @title SVM using freq
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

X = df[['Positive_Freq', 'Negative_Freq']]
y = df['Class']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Creating an SVM classifier
svm = SVC(kernel='linear', C=1.0, random_state=42)

# Training the SVM model
svm.fit(X_train, y_train)

# Predicting on the test set
y_pred = svm.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Classification report
print(classification_report(y_test, y_pred))

# @title making a WORD format
from nltk.tokenize import word_tokenize
import pandas as pd
import os

# Define paths to your positive and negative folders
positive_folder = '/content/txt_sentoken/pos'
negative_folder = '/content/txt_sentoken/neg'

# Function to extract tokens from a file
def extract_tokens(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        content = file.read()
        tokens = word_tokenize(content)
        return tokens

# Extracting all unique tokens from multiple files
all_tokens = set()
for folder in [positive_folder, negative_folder]:
    for file in os.listdir(folder):
        file_path = os.path.join(folder, file)
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            tokens = word_tokenize(content)
            all_tokens.update(tokens)

# Separate positive and negative tokens
positive_tokens = set(opinion_lexicon.positive()).intersection(all_tokens)
negative_tokens = set(opinion_lexicon.negative()).intersection(all_tokens)

# Create DataFrame columns with positive and negative tokens
columns = list(positive_tokens) + list(negative_tokens)

# Creating a DataFrame to store count of positive and negative tokens for each file
file_paths = [os.path.join(positive_folder, file) for file in os.listdir(positive_folder)] + \
             [os.path.join(negative_folder, file) for file in os.listdir(negative_folder)]
count_data = []
for file_path in file_paths:
    tokens_in_file = extract_tokens(file_path)
    positive_tokens_in_file = [token for token in tokens_in_file if token in positive_tokens]
    negative_tokens_in_file = [token for token in tokens_in_file if token in negative_tokens]
    count_vector = [positive_tokens_in_file.count(token) for token in columns[:len(positive_tokens)]] + \
                   [negative_tokens_in_file.count(token) for token in columns[len(positive_tokens):]]
    # Determine classifier (1 for positive files, 0 for negative files)
    classifier = 1 if file_path.startswith(positive_folder) else 0
    count_data.append(count_vector + [classifier])

# Convert count vectors into a DataFrame
columns.append('Class')
count_df = pd.DataFrame(count_data, columns=columns)

# Displaying the resulting DataFrame
print(count_df)

# @title Dont worry ,actually this dataframe have 1's , check it here
ones_locations = count_df[count_df == 1].stack().index.tolist()

if ones_locations:
    print("Locations of '1's in the DataFrame:")
    for row, col in ones_locations:
        print(f"Row: {row}, Column: {col}")
else:
    print("There are no '1's in the DataFrame.")

# @title naive bayes with word
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score,classification_report

# Splitting the data into train and test sets
X = count_df.drop('Class', axis=1)
y = count_df['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initializing and fitting the Naive Bayes classifier
naive_bayes_word = MultinomialNB()
naive_bayes_word.fit(X_train, y_train)

# Predicting on the test set
y_pred = naive_bayes_word.predict(X_test)

# Calculating accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Naive Bayes Accuracy:", accuracy)

# Classification report
print(classification_report(y_test, y_pred))

# @title MAXENT with word
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

X = count_df.drop('Class', axis=1)
y = count_df['Class']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Creating a Logistic Regression classifier (MaxEnt)
maxent_word = LogisticRegression(max_iter=1000)  # Increase max_iter if needed

# Training the MaxEnt model
maxent_word.fit(X_train, y_train)

# Predicting on the test set
y_pred = maxent_word.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Classification report
print(classification_report(y_test, y_pred))

# @title SVM using word
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

X = count_df.drop('Class', axis=1)
y = count_df['Class']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Creating an SVM classifier
svm_word = SVC(kernel='linear', C=1.0, random_state=42)

# Training the SVM model
svm_word.fit(X_train, y_train)

# Predicting on the test set
y_pred = svm_word.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Classification report
print(classification_report(y_test, y_pred))

# @title making this in TF_IDF format data
from sklearn.feature_extraction.text import TfidfVectorizer

#function for making corpus of only postive and negative tokens
def make_corpus(text, positive_criteria, negative_criteria):
    corpus = ''
    tokens = word_tokenize(text)
    for token in tokens:
       if token.lower() in (positive_criteria | negative_criteria):
          corpus += token + ' '  # Concatenate each token with a space
    return corpus.rstrip()  # Remove trailing space before returning

# Paths to your positive and negative folders
positive_folder = '/content/txt_sentoken/pos'
negative_folder = '/content/txt_sentoken/neg'

# Use NLTK's opinion_lexicon positive and negative words as criteria
positive_criteria = set(opinion_lexicon.positive())
negative_criteria = set(opinion_lexicon.negative())

# Create lists to store positive and negative frequencies for each document
corpus = []
class_labels = []

# Calculate positive and negative frequencies for each document in the positive folder
for file_name in os.listdir(positive_folder):
    with open(os.path.join(positive_folder, file_name), 'r', encoding='utf-8') as file:
        content = file.read()
        current_corpus = make_corpus(content, positive_criteria, negative_criteria)
        if current_corpus.strip():  # Check if the current corpus is not empty
            corpus.append(current_corpus)
            class_labels.append(1)  # Class label 1 for positive documents

# Calculate positive and negative frequencies for each document in the negative folder
for file_name in os.listdir(negative_folder):
    with open(os.path.join(negative_folder, file_name), 'r', encoding='utf-8') as file:
        content = file.read()
        current_corpus = make_corpus(content, positive_criteria, negative_criteria)
        if current_corpus.strip():  # Check if the current corpus is not empty
            corpus.append(current_corpus)
            class_labels.append(0)  # Class label 0 for negative documents


tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(corpus)

# Convert TF-IDF matrix to a DataFrame
feature_names = tfidf.get_feature_names_out()
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)
df_tfidf['Class'] = class_labels  # Add the class labels as a column


print(df_tfidf)

# @title Dont worry actually this data frame have values > 0, check it here
ones_locations = df_tfidf[df_tfidf > 0].stack().index.tolist()

if ones_locations:
    print("Locations of '1's in the DataFrame:")
    for row, col in ones_locations:
        print(f"Row: {row}, Column: {col}")
else:
    print("There are no '1's in the DataFrame.")

# @title naive bayes with TF_IDF
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score,classification_report

# Splitting the data into train and test sets
X = df_tfidf.drop('Class', axis=1)
y = df_tfidf['Class']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initializing and fitting the Naive Bayes classifier
naive_bayes_tfidf = MultinomialNB()
naive_bayes_tfidf.fit(X_train, y_train)

# Predicting on the test set
y_pred = naive_bayes_tfidf.predict(X_test)

# Calculating accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Naive Bayes Accuracy:", accuracy)

# Classification report
print(classification_report(y_test, y_pred))

# @title MAXENT with TF-IDF
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

X = df_tfidf.drop('Class', axis=1)
y = df_tfidf['Class']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Creating a Logistic Regression classifier (MaxEnt)
maxent_tfidf = LogisticRegression(max_iter=1000)  # Increase max_iter if needed

# Training the MaxEnt model
maxent_tfidf.fit(X_train, y_train)

# Predicting on the test set
y_pred = maxent_tfidf.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Classification report
print(classification_report(y_test, y_pred))

# @title SVM using TF-IDF
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report

X = df_tfidf.drop('Class', axis=1)
y = df_tfidf['Class']

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Creating an SVM classifier
svm_tfidf = SVC(kernel='linear', C=1.0, random_state=42)

# Training the SVM model
svm_tfidf.fit(X_train, y_train)

# Predicting on the test set
y_pred = svm_tfidf.predict(X_test)

# Evaluating the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Classification report
print(classification_report(y_test, y_pred))